# Function Approximation Using Backpropagation

This project involves using backpropagation to approximate mathematical functions such as sine, cosine, logarithm, and user-defined functions. The goal is to train neural networks to learn these functions accurately through iterative learning.

## What is Backpropagation?

Backpropagation is a key algorithm used for training artificial neural networks. It calculates the gradient of the loss function with respect to each weight in the network, allowing the model to update weights efficiently. The process involves two main phases:

1. **Forward Pass**: The input is propagated through the network to produce an output.
2. **Backward Pass**: The error between the predicted output and the actual output is propagated back through the network to adjust weights using gradient descent.

This iterative process continues until the model accurately approximates the target function.

## Functions Covered

- **Sine Function**
- **Cosine Function**
- **Logarithmic Function**
- **User-Defined Functions**

## How to Run

1. Clone the repository and set up the environment.
2. Define the function you want to approximate or select from the existing functions.
3. Run the training script to see how the model approximates the function.

## Conclusion

This project demonstrates how neural networks can be effectively used for function approximation using the backpropagation algorithm, highlighting its versatility in solving complex mathematical problems.
